---
layout: about
title: about
permalink: /
subtitle: Postdoctoral Research Fellow in <a href="https://www.yonsei.ac.kr/en_sc/" class="page-description" target="_blank">Yonsei University</a>.

profile:
  align: right
  image: profile.jpg

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
service: true  # includes service section
social: true  # includes social icons at the bottom of the page
---
I am a Postdoctoral Fellow at the [Seamless Trans-X Lab (STL)](http://stl.yonsei.ac.kr/), [Yonsei University](https://www.yonsei.ac.kr/en_sc/), working on multispectral imaging and robust visual perception for autonomous vehicles. My research lies at the intersection of computer vision, sensor fusion, and intelligent mobility systems. 

I received my Ph.D. in Computer Science from the [Izmir Institute of Technology](https://en.iyte.edu.tr/), where my dissertation focused on vehicle maneuver detection for advanced driver assistance systems (ADAS) under the supervision of [Prof. Dr. Yalin Bastanlar](https://ceng.iyte.edu.tr/people/yalin-bastanlar/). My doctoral research focused on vehicle maneuver detection for advanced driver assistance systems (ADAS). During my doctoral studies, I also spent a term as a visiting researcher at [Seoul National University](https://en.snu.ac.kr/)'s Vehicle Dynamics and Control Laboratory ([VDCL](https://vdcl.snu.ac.kr/)) & the Future Mobility Technology Center ([FMTC](https://fmtc.snu.ac.kr/)) supported by a research scholarship from TUBITAK (The Scientific and Technological Research Council of Turkey).
<br><br>
My current research interests revolve around computer vision and deep learning, with a specialization in multispectral camera systems, perception in adverse weather scenarios, representation learning, and vision-language modeling.
<br><br>
My current research include:
<br>
* **Multispectral perception:** designing datasets and architectures that fuse visible and infrared modalities for detection, segmentation, and depth estimation.
* **Image translation and representation learning:** leveraging Vision Foundation Models for RGB-to-Multispectral domain adaptation and spectral translation.
* **Vision-language and explainable perception:** exploring how multimodal models can interpret and describe difficult to understand scenes for safe decision-making.